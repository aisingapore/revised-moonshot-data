from pathlib import Path
from typing import Any

from moonshot.src.metrics.metric_interface import MetricInterface
from moonshot.src.utils.log import configure_logger
from moonshot.src.utils.timeit import timeit
from rouge_score import rouge_scorer

# Create a logger for this module
logger = configure_logger(__name__)


class RougeScorer(MetricInterface):
    def __init__(self):
        self.id = Path(__file__).stem
        self.name = "RougeScorer"
        self.description = "RougeScorer returns the various rouge scores."
        self.metric_config = self.get_metrics_configuration(self.id)
        self.endpoints = self.metric_config.get("endpoints", [])
        self.configurations = self.metric_config.get("configurations", {})

    def get_metadata(self) -> dict | None:
        """
        Retrieves and returns the metadata of the RougeScorer class.

        Returns:
            dict | None: A dictionary containing the 'id', 'name', 'description', 'endpoints' and 'configurations'
            of the RougeScorer class, or None if not applicable.
        """
        return {
            "id": self.id,
            "name": self.name,
            "description": self.description,
            "endpoints": self.endpoints,
            "configurations": self.configurations,
        }

    @timeit
    async def get_results(
        self, prompts: Any, predicted_results: Any, targets: Any, *args, **kwargs
    ) -> dict:
        """
        Computes the ROUGE scores for a set of predicted results compared to target outputs.

        Args:
            prompts (Any): The input prompts used to generate the predicted results.
            predicted_results (Any): The predicted results generated by the model, each containing a response attribute.
            targets (Any): The target outputs for comparison with the predicted results.
            *args: Additional positional arguments.
            **kwargs: Additional keyword arguments.

        Returns:
            dict: A dictionary containing:
                - "rougescorer": A dictionary with:
                    - "score": Average ROUGE scores across all samples.
                    - "individual_scores": A list of dictionaries for each sample containing:
                        - "prompt": The input prompt.
                        - "predicted_value": The predicted result.
                        - "target": The target result.
                        - "score": A dictionary with individual ROUGE scores (recall, precision, f-measure).
                - "grading_criteria": An empty dictionary for grading criteria.

        Raises:
            RuntimeError: If an error occurs during the calculation of the ROUGE scores.
        """
        predicted_values = [result.response for result in predicted_results]

        try:
            # Define the test metrics to calculate
            test_metrics = ["rouge1", "rouge2", "rougeLsum"]

            # Initialize average recall, precision, and f-measure lists
            avg_recall = [0.0, 0.0, 0.0]
            avg_precision = [0.0, 0.0, 0.0]
            avg_fmeasure = [0.0, 0.0, 0.0]

            # Initialize the output dictionary and individual scores list
            output_dict = {}
            individual_scores = []

            # Calculate rouge scores for each target-output pair
            logger.info("[RougeScorer] Attempting to calculate rouge score")
            scorer = rouge_scorer.RougeScorer(test_metrics)
            for prompt, target, result in zip(prompts, targets, predicted_values):
                scores = scorer.score(target, result)
                test_metrics_dict = {}
                for test_metric_index, test_metric in enumerate(test_metrics, 0):
                    # Store each individual rouge score to calculate average score
                    avg_recall[test_metric_index] += scores[test_metric].recall
                    avg_precision[test_metric_index] += scores[test_metric].precision
                    avg_fmeasure[test_metric_index] += scores[test_metric].fmeasure

                    # Store each individual rouge score
                    test_metrics_dict[test_metric] = {
                        "recall": scores[test_metric].recall,
                        "precision": scores[test_metric].precision,
                        "fmeasure": scores[test_metric].fmeasure,
                    }

                # Add prompt, predicted value, and target to individual scores
                individual_scores.append(
                    {
                        "prompt": prompt,
                        "predicted_value": result,
                        "target": target,
                        "score": test_metrics_dict,
                    }
                )

            # Calculate average scores and add them to the output dictionary
            for avg_index, (recall, precision, fmeasure) in enumerate(
                zip(avg_recall, avg_precision, avg_fmeasure)
            ):
                output_dict[f"avg_{test_metrics[avg_index]}"] = {
                    "recall": recall / len(targets),
                    "precision": precision / len(targets),
                    "fmeasure": fmeasure / len(targets),
                }

            # Return the final rouge scores dictionary
            return {
                "rougescorer": {
                    "score": output_dict,
                    "individual_scores": individual_scores,
                },
                "grading_criteria": {},
            }

        except Exception as exception:
            # Raise an error if there is an exception during calculation
            raise RuntimeError(f"Unable to calculate rouge score - {str(exception)}")
